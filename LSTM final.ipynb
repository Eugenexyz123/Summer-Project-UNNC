{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9618a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(839702, 152)\n",
      "(140196, 152)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/nn.py:827: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.5406 - loss: 0.6901\n",
      "Epoch 2/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5501 - loss: 0.6853\n",
      "Epoch 3/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5475 - loss: 0.6854\n",
      "Epoch 4/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5422 - loss: 0.6872\n",
      "Epoch 5/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5255 - loss: 0.6891\n",
      "Epoch 6/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5368 - loss: 0.6886\n",
      "Epoch 7/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5447 - loss: 0.6873\n",
      "Epoch 8/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5329 - loss: 0.6872\n",
      "Epoch 9/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5298 - loss: 0.6897\n",
      "Epoch 10/10\n",
      "\u001b[1m2180/2180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.5469 - loss: 0.6864\n",
      "<Functional name=functional_2, built=True>\n",
      "\u001b[1m 184/4360\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 548us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/nn.py:827: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4360/4360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step\n",
      "Confusion Matrix:\n",
      "[[ 64 268]\n",
      " [ 39 330]]\n",
      "准确率：0.5621\n",
      "精确率：0.5518\n",
      "召回率：0.8943\n",
      "二分类F1值：0.6825\n",
      "AUC: 0.5435\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Flatten, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def LSTM_Attention_model (X_train, y_train, epochs=10, batch_size=64, verbose=1) :\n",
    "    inputs = Input(shape=(X_train.shape[1],X_train.shape[-1]))\n",
    "    lstm_out = LSTM (128, return_sequences=True) (inputs)\n",
    "    attention_out = Attention () ([lstm_out, lstm_out])\n",
    "    lstm2 = LSTM(64, return_sequences=True) (attention_out)\n",
    "    flattened_out = Flatten () (lstm2)\n",
    "    output2 = Dense (25, activation='sigmoid') (flattened_out)\n",
    "    output = Dense (1, activation='sigmoid') (output2)\n",
    "\n",
    "    model = Model (inputs=inputs, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "    model.fit (X_train, y_train, epochs=epochs, batch_size=batch_size,verbose=1)\n",
    "    return model\n",
    "\n",
    "#Import the final dataset\n",
    "\n",
    "df0=pd.read_csv('finaldata.csv')\n",
    "df0.fillna(0, inplace=True)\n",
    "print(df0.shape)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Divide the majority class and minority class\n",
    "majority_class = df0[df0['OS_P'] == 1]\n",
    "minority_class = df0[df0['OS_P'] == 0]\n",
    "\n",
    "# Randomly sample the data in majority calss  such that the samples in it is 3 times of the ones in minority class\n",
    "n_samples = len(minority_class)\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=n_samples, random_state=42)\n",
    "\n",
    "# Combine the two classes od data\n",
    "df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "\n",
    "X = df.drop('OS_P', axis=1).values\n",
    "y = df['OS_P'].values\n",
    "\n",
    "#Adjust the data into the appropriate scale\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_for_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Split the dataset into training set and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.005, random_state=42)\n",
    "\n",
    "#Adjust the data format into the the required one \n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Construct the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(1,X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "md = LSTM_Attention_model (X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "print(md)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the LSTM model to generate features\n",
    "X_train_features = md.predict(X_train)\n",
    "X_test_features = md.predict(X_test)\n",
    "\n",
    "# Define the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier using the features generated by the LSTM model\n",
    "rf.fit(X_train_features, y_train)\n",
    "\n",
    "# Predict the classes\n",
    "y_pred = rf.predict(X_test_features)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Compute the accuracy\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score (y_test, y_pred)\n",
    "binary_f1 = f1_score (y_test, y_pred)\n",
    "# Step 5: AUC\n",
    "auc = roc_auc_score (y_test, y_pred)\n",
    "\n",
    "print('Accuracy：{:.4f}'.format(accuracy))\n",
    "print('Precision：{:.4f}'.format(precision))\n",
    "print('Recall：{:.4f}'.format(recall))\n",
    "print('F1：{:.4f}'.format(binary_f1))\n",
    "print ('AUC: {:.4f}'.format (auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fc6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biLSTM_Attention_model (X_train, y_train, epochs=10, batch_size=64, verbose=1) :\n",
    "    inputs = Input(shape=(X_train.shape[1],X_train.shape[-1]))\n",
    "    bilstm_out =  Bidirectional(LSTM(128,return_sequences=True))(inputs)\n",
    "    attention_out = Attention () ([bilstm_out, bilstm_out])\n",
    "    bilstm2 = Bidirectional(LSTM(64, return_sequences=True))(attention_out)\n",
    "    flattened_out = Flatten () (bilstm2)\n",
    "    output2 = Dense (25, activation='sigmoid') (flattened_out)\n",
    "    output = Dense (1, activation='sigmoid') (output2)\n",
    "\n",
    "    model = Model (inputs=inputs, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "    model.fit (X_train, y_train, epochs=epochs, batch_size=batch_size,verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479b14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8630c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558b940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13044037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13539383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
